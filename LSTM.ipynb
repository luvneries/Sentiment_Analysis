{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with Deep learning framework for Sentiment Analysis\n",
    "### NLP\n",
    "\n",
    "NLP is technique to create system which can process or understand language on order to perform certain tasks. These task could be:\n",
    "\n",
    "* Question Answering - The main job of technologies like Siri, Alexa, and Cortana\n",
    "* Sentiment Analysis - Determining the emotional tone behind a piece of text\n",
    "* Image to Text Mappings - Generating a caption for an input image\n",
    "* Machine Translation - Translating a paragraph of text to another language\n",
    "* Speech Recognition - Having computers recognize spoken words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is an excercise of taking sentence, paragraph, document or any piece of natural language and determine text emotional tone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/SentimentAnalysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets understand how things works in system\n",
    "\n",
    "System/CPU understands numerical/scalar values for computation.\n",
    "* CNN uses array of pixel values\n",
    "* logistics/linear regression uses quantifable features\n",
    "* NLP uses vector respresentation of each word.\n",
    "\n",
    "![caption](Images/SentimentAnalysis2.png)\n",
    "\n",
    "\n",
    "These vectors to be created in such a way that it represents it context and meaning. Similar words should reside relatively in same area.\n",
    "\n",
    "![caption](Images/SentimentAnalysis8.png)\n",
    "\n",
    "These vector representations are called as word embeddings.\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "Word2Vec model is used to create those word embeddings.\n",
    "The Word2Vec model is trained by taking each sentence in the dataset, sliding a window of fixed size over it, and trying to predict the center word of the window, given the other words. Using a loss function and optimization procedure, the model generates vectors for each unique word.\n",
    "\n",
    "NLP with Deep learning will likely to have word vectors as input.\n",
    "Lets start our practical excercise to get better understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Deep Learning\n",
    "\n",
    "As mentioned before, the task of sentiment analysis involves taking in an input sequence of words and determining whether the sentiment is positive, negative, or neutral. We can separate this specific task (and most other NLP tasks) into 5 different components.\n",
    "\n",
    "    1) Training a word vector generation model (such as Word2Vec) or loading pretrained word vectors\n",
    "    2) Creating an ID's matrix for our training set (We'll discuss this a bit later)\n",
    "    3) RNN (With LSTM units) graph creation\n",
    "    4) Training \n",
    "    5) Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "\n",
    "Traning Word2Vec model will take lot of time so we will use a pretrained model. Google provides word2vec model with 100 billion different words :-) which contains 3 million words with a dimensionality of 300.\n",
    "\n",
    "Another is Glove with 0.4 million with dimensionalty of 50.\n",
    "\n",
    "We will be importing Glove data as list and embedding matrix for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "wordsList = np.load('training_data/wordsList.npy')\n",
    "wordsList = wordsList.tolist()\n",
    "wordsList = [word.decode('UTF-8') for word in wordsList]\n",
    "wordVectors = np.load('training_data/wordVectors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordsList:  400000 \n",
      "wordVectors:  (400000, 50)\n",
      "222\n",
      "['going', 'number', 'major', 'known', 'points', 'won']\n"
     ]
    }
   ],
   "source": [
    "print('wordsList: ', len(wordsList), '\\nwordVectors: ', wordVectors.shape)\n",
    "print(wordsList.index(\"going\"))\n",
    "print(wordsList[222:228])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35413944721221924"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordVectors[wordsList.index(\"going\")],wordVectors[wordsList.index(\"walking\")]\n",
    "\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "cosine(wordVectors[wordsList.index(\"love\")],wordVectors[wordsList.index(\"hate\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[    41    804 201534   1005     15   7446      5  13767      0      0]\n"
     ]
    }
   ],
   "source": [
    "sentence = 'i thought the movie was incredible and inspiring'\n",
    "import tensorflow as tf\n",
    "maxSeqLength = 10\n",
    "firstSentence = np.zeros((maxSeqLength), dtype = 'int32')\n",
    "#firstSentence[0] = wordsList.index(i)\n",
    "for i, word in enumerate(sentence.split()):\n",
    "    firstSentence[i] = wordsList.index(word)\n",
    "print(firstSentence.shape)\n",
    "print(firstSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.18910000e-01   1.52549997e-01  -8.20730031e-02  -7.41439998e-01\n",
      "    7.59169996e-01  -4.83280003e-01  -3.10090005e-01   5.14760017e-01\n",
      "   -9.87079978e-01   6.17570011e-04  -1.50429994e-01   8.37700009e-01\n",
      "   -1.07969999e+00  -5.14599979e-01   1.31879997e+00   6.20069981e-01\n",
      "    1.37789994e-01   4.71080005e-01  -7.28740022e-02  -7.26750016e-01\n",
      "   -7.41159976e-01   7.52629995e-01   8.81799996e-01   2.95610011e-01\n",
      "    1.35479999e+00  -2.57010007e+00  -1.35230005e+00   4.58799988e-01\n",
      "    1.00680006e+00  -1.18560004e+00   3.47370005e+00   7.78980017e-01\n",
      "   -7.29290009e-01   2.51020014e-01  -2.61559993e-01  -3.46839994e-01\n",
      "    5.58409989e-01   7.50980020e-01   4.98299986e-01  -2.68229991e-01\n",
      "   -2.74430006e-03  -1.82980001e-02  -2.80959994e-01   5.53179979e-01\n",
      "    3.77059989e-02   1.85550004e-01  -1.50250003e-01  -5.75119972e-01\n",
      "   -2.66710013e-01   9.21209991e-01]\n",
      " [  4.27619994e-01  -1.14689998e-01   1.05060004e-02  -5.46620011e-01\n",
      "    8.90550017e-01   1.92629993e-01  -6.53739989e-01   8.74610022e-02\n",
      "   -6.98300004e-01   2.80200005e-01   1.71759993e-01   3.18859994e-01\n",
      "   -4.62529987e-01  -1.34140000e-01   6.20700002e-01   3.36030006e-01\n",
      "    4.77930009e-01  -4.68610004e-02  -4.51790005e-01  -3.27650011e-01\n",
      "   -7.30170012e-01   4.14490014e-01   5.67830026e-01   3.80100012e-02\n",
      "    1.01400006e+00  -1.88499999e+00  -9.44019973e-01   6.50020018e-02\n",
      "    5.49920022e-01  -4.69390005e-01   2.72340012e+00  -1.40709996e-01\n",
      "   -7.80159980e-02  -8.14239979e-01  -6.64139986e-02  -4.23359990e-01\n",
      "    7.89779983e-03   4.07579988e-01   2.12449998e-01   1.01499997e-01\n",
      "   -3.72049987e-01   1.81990005e-02  -8.11730027e-02   7.69240022e-01\n",
      "    3.03669989e-01   5.12220003e-02  -2.38560006e-01  -3.45349982e-02\n",
      "   -4.12670001e-02   2.59400010e-01]\n",
      " [  4.18000013e-01   2.49679998e-01  -4.12420005e-01   1.21699996e-01\n",
      "    3.45270008e-01  -4.44569997e-02  -4.96879995e-01  -1.78619996e-01\n",
      "   -6.60229998e-04  -6.56599998e-01   2.78430015e-01  -1.47670001e-01\n",
      "   -5.56770027e-01   1.46579996e-01  -9.50950012e-03   1.16579998e-02\n",
      "    1.02040000e-01  -1.27920002e-01  -8.44299972e-01  -1.21809997e-01\n",
      "   -1.68009996e-02  -3.32789987e-01  -1.55200005e-01  -2.31309995e-01\n",
      "   -1.91809997e-01  -1.88230002e+00  -7.67459989e-01   9.90509987e-02\n",
      "   -4.21249986e-01  -1.95260003e-01   4.00710011e+00  -1.85939997e-01\n",
      "   -5.22870004e-01  -3.16810012e-01   5.92130003e-04   7.44489999e-03\n",
      "    1.77780002e-01  -1.58969998e-01   1.20409997e-02  -5.42230010e-02\n",
      "   -2.98709989e-01  -1.57490000e-01  -3.47579986e-01  -4.56370004e-02\n",
      "   -4.42510009e-01   1.87849998e-01   2.78489990e-03  -1.84110001e-01\n",
      "   -1.15139998e-01  -7.85809994e-01]\n",
      " [  3.08239996e-01   1.72230005e-01  -2.33390003e-01   2.31049992e-02\n",
      "    2.85219997e-01   2.30759993e-01  -4.10479993e-01  -1.00349998e+00\n",
      "   -2.07200006e-01   1.43270004e+00  -8.06840003e-01   6.89540029e-01\n",
      "   -4.36479986e-01   1.10689998e+00   1.61070001e+00  -3.19660008e-01\n",
      "    4.77440000e-01   7.93950021e-01  -8.43739986e-01   6.45089969e-02\n",
      "    9.02509987e-01   7.86090016e-01   2.96990007e-01   7.60569990e-01\n",
      "    4.32999998e-01  -1.50320005e+00  -1.64230001e+00   3.02560002e-01\n",
      "    3.07709992e-01  -8.70570004e-01   2.47819996e+00  -2.58520003e-02\n",
      "    5.01299977e-01  -3.85930002e-01  -1.56330004e-01   4.55220014e-01\n",
      "    4.90100011e-02  -4.25989985e-01  -8.64019990e-01  -1.30760002e+00\n",
      "   -2.95760006e-01   1.20899999e+00  -3.12700003e-01  -7.24619985e-01\n",
      "   -8.08009982e-01   8.26670006e-02   2.67379999e-01  -9.81769979e-01\n",
      "   -3.21469992e-01   9.98229980e-01]\n",
      " [  8.68880004e-02  -1.94159999e-01  -2.42670000e-01  -3.33909988e-01\n",
      "    5.67309976e-01   3.97830009e-01  -9.78089988e-01   3.15899998e-02\n",
      "   -6.14690006e-01  -3.14060003e-01   5.61450005e-01   1.28859997e-01\n",
      "   -8.41929972e-01  -4.69920009e-01   4.70970005e-01   2.30119992e-02\n",
      "   -5.96090019e-01   2.22910002e-01  -1.16139996e+00   3.86500001e-01\n",
      "    6.74119964e-02   4.48830009e-01   1.73940003e-01  -5.35740018e-01\n",
      "    1.79089993e-01  -2.16470003e+00  -1.28270000e-01   2.90360004e-01\n",
      "   -1.50610000e-01   3.52420002e-01   3.12400007e+00  -9.00849998e-01\n",
      "   -2.56699994e-02  -4.17089999e-01   4.05649990e-01  -2.27029994e-01\n",
      "    7.68289983e-01   6.09820008e-01   7.00680017e-02  -1.32709995e-01\n",
      "   -1.20099999e-01   9.61320028e-02  -4.39980000e-01  -4.85309988e-01\n",
      "   -5.18800020e-01  -3.07700008e-01  -7.50280023e-01  -7.69999981e-01\n",
      "    3.94499987e-01  -1.69369996e-01]\n",
      " [ -1.46890000e-01   5.42320013e-01  -1.13749998e-02   2.91619986e-01\n",
      "    7.99099982e-01  -6.10870011e-02   4.41890001e-01   1.89099997e-01\n",
      "    5.48860013e-01   1.19319999e+00  -2.67710000e-01   6.40079975e-02\n",
      "   -6.95190012e-01  -5.17870009e-01   6.71750009e-01  -7.05089986e-01\n",
      "    5.16420007e-01   5.40560007e-01  -7.94399977e-01  -8.01880002e-01\n",
      "   -4.85480011e-01   7.92169988e-01  -2.15820000e-01  -1.04159999e+00\n",
      "    1.25460005e+00  -2.75790006e-01  -1.42429996e+00  -6.23399988e-02\n",
      "    1.23049998e+00   6.64360002e-02   1.73580003e+00   8.83210003e-01\n",
      "    4.92139995e-01  -3.44069988e-01  -2.88219988e-01   4.34350014e-01\n",
      "   -2.44369999e-01   2.87339985e-01   2.02810001e-02  -5.60869992e-01\n",
      "   -1.89400002e-01  -2.62219995e-01  -4.76130009e-01   1.48259997e-01\n",
      "   -4.27789986e-01   1.02519996e-01   2.26170003e-01   1.46009997e-01\n",
      "   -5.07390015e-02   3.62549990e-01]\n",
      " [  2.68180013e-01   1.43460006e-01  -2.78770000e-01   1.62569992e-02\n",
      "    1.13839999e-01   6.99230015e-01  -5.13320029e-01  -4.73679990e-01\n",
      "   -3.30749989e-01  -1.38339996e-01   2.70200014e-01   3.09379995e-01\n",
      "   -4.50120002e-01  -4.12699997e-01  -9.93200019e-02   3.80849987e-02\n",
      "    2.97490004e-02   1.00759998e-01  -2.50580013e-01  -5.18180013e-01\n",
      "    3.45580012e-01   4.49220002e-01   4.87910002e-01  -8.08660015e-02\n",
      "   -1.01209998e-01  -1.37769997e+00  -1.08659998e-01  -2.32010007e-01\n",
      "    1.28389997e-02  -4.65079993e-01   3.84629989e+00   3.13620001e-01\n",
      "    1.36429995e-01  -5.22440016e-01   3.30199987e-01   3.37069988e-01\n",
      "   -3.56009990e-01   3.24310005e-01   1.20410003e-01   3.51200014e-01\n",
      "   -6.90430030e-02   3.68849993e-01   2.51679987e-01  -2.45169997e-01\n",
      "    2.53809988e-01   1.36700004e-01  -3.11780006e-01  -6.32099986e-01\n",
      "   -2.50279993e-01  -3.80970001e-01]\n",
      " [  1.96459994e-01   5.19469976e-01  -1.77900001e-01  -6.19279981e-01\n",
      "    5.78279972e-01  -7.79249966e-02   1.32939994e-01  -1.72889993e-01\n",
      "   -3.37769985e-01   1.46720004e+00  -7.98169971e-01  -1.95099995e-01\n",
      "   -1.07220002e-01   2.14600004e-03  -1.23369999e-01  -9.35559988e-01\n",
      "    5.28039992e-01   4.28770006e-01   4.52480018e-02  -1.13679998e-01\n",
      "   -2.46360004e-01   7.15799987e-01  -3.42350006e-01  -2.86529996e-02\n",
      "    1.31130004e+00  -3.87199998e-01  -1.08360004e+00  -3.14020008e-01\n",
      "    3.90469998e-01   1.49020001e-01   1.12290001e+00   6.49829984e-01\n",
      "    2.41150007e-01  -9.01579976e-01  -2.81590015e-01   2.67500013e-01\n",
      "   -3.68809998e-01   3.27910006e-01  -9.54190016e-01  -4.06549990e-01\n",
      "   -9.79469996e-03  -2.62109995e-01  -4.39779997e-01  -6.72369972e-02\n",
      "    5.13580024e-01   5.98349988e-01  -9.83439982e-02   2.17409998e-01\n",
      "    3.71440016e-02   1.25210002e-01]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(tf.nn.embedding_lookup(wordVectors, firstSentence).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](Images/SentimentAnalysis5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "positiveFiles = ['training_data/positiveReviews/' + f for f in listdir('training_data/positiveReviews/')]\n",
    "negativeFiles = ['training_data/negativeReviews/' + f for f in listdir('training_data/negativeReviews/')]\n",
    "\n",
    "numWords = []\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, 'r', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, 'r', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 2470 10\n"
     ]
    }
   ],
   "source": [
    "numFiles = len(numWords)\n",
    "print(numFiles, max(numWords), min(numWords))\n",
    "#print(numWords[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in line is 233\n"
     ]
    }
   ],
   "source": [
    "print('Average number of words in line is {}'.format(int(sum(numWords)/len(numWords))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11163ad30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.hist(numWords, bins=50)\n",
    "plt.xlabel(\"Sequence of Words\", fontsize = 12)\n",
    "plt.ylabel(\"Count of words\", fontsize=12)\n",
    "plt.title(\"Distribution of length of sentence\")\n",
    "plt.axis([0,1200,0,8000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's a strange feeling to sit alone in a theater occupied by parents and their rollicking kids. I felt like instead of a movie ticket, I should have been given a NAMBLA membership.<br /><br />Based upon Thomas Rockwell's respected Book, How To Eat Fried Worms starts like any children's story: moving to a new town. The new kid, fifth grader Billy Forrester was once popular, but has to start anew. Making friends is never easy, especially when the only prospect is Poindexter Adam. Or Erica, who at 4 1/2 feet, is a giant.<br /><br />Further complicating things is Joe the bully. His freckled face and sleeveless shirts are daunting. He antagonizes kids with the Death Ring: a Crackerjack ring that is rumored to kill you if you're punched with it. But not immediately. No, the death ring unleashes a poison that kills you in the eight grade.<br /><br />Joe and his axis of evil welcome Billy by smuggling a handful of slimy worms into his thermos. Once discovered, Billy plays it cool, swearing that he eats worms all the time. Then he throws them at Joe's face. Ewww! To win them over, Billy reluctantly bets that he can eat 10 worms. Fried, boiled, marinated in hot sauce, squashed and spread on a peanut butter sandwich. Each meal is dubbed an exotic name like the \"Radioactive Slime Delight,\" in which the kids finally live out their dream of microwaving a living organism.<br /><br />If you've ever met me, you'll know that I have an uncontrollably hearty laugh. I felt like a creep erupting at a toddler whining that his \"dilly dick\" hurts. But Fried Worms is wonderfully disgusting. Like a G-rated Farrelly brothers film, it is both vomitous and delightful.<br /><br />Writer/director Bob Dolman is also a savvy storyteller. To raise the stakes the worms must be consumed by 7 pm. In addition Billy holds a dark secret: he has an ultra-sensitive stomach.<br /><br />Dolman also has a keen sense of perspective. With such accuracy, he draws on children's insecurities and tendency to exaggerate mundane dilemmas.<br /><br />If you were to hyperbolize this movie the way kids do their quandaries, you will see that it is essentially about war. Freedom-fighter and freedom-hater use pubescent boys as pawns in proxy wars, only to learn a valuable lesson in unity. International leaders can learn a thing or two about global peacekeeping from Fried Worms.<br /><br />At the end of the film, I was comforted when two chaperoning mothers behind me, looked at each other with befuddlement and agreed, \"That was a great movie.\" Great, now I won't have to register myself in any lawful databases.\n"
     ]
    }
   ],
   "source": [
    "fname = positiveFiles[3]\n",
    "with open(fname, 'r') as f:\n",
    "    for lines in f:\n",
    "        print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "\n",
    "def cleanSentences(string):\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    47,      7,   5186,   2518,      4,   3162,   1485,      6,\n",
       "            7,   2248,   3001,     21,   1108,      5,     44,  48222,\n",
       "         1813,     41,   1349,    117,    773,      3,      7,   1005,\n",
       "         3317,     41,    189,     33,     51,    454,      7, 166008,\n",
       "         2798,    243,   1219,   1160, 399999,   5313,    539,    197,\n",
       "            4,   3623,  10503,  16632,   2383,    117,    130,  53362,\n",
       "          523,   1233,      4,      7,     50,    328, 201534,     50,\n",
       "         4313,   1239,  20155,   4785,  19798,     15,    442,    814,\n",
       "           34,     31,      4,    465,  16972,    433,   1095,     14,\n",
       "          332,   1673,    858,     61, 201534,     91,   4251,     14,\n",
       "        54048,   3926,     46,  23842,     38,     22,    409,    421,\n",
       "         1237,     14,      7,   1752,    489,  20773,    654,     14,\n",
       "         1984, 201534,  17224,     26,  91953,    621,      5,  44864,\n",
       "         6162,     32,  13073,     18, 167360,   1813,     17, 201534,\n",
       "          336,   2930,      7, 110855,   2930,     12,     14,  16215,\n",
       "            4,   1916,     81,     83, 211666,  14019,     17,     20,\n",
       "           34,     36,   1040,     84, 201534,    336,   2930,  53948,\n",
       "            7,   9373,     12,   5059,     81,      6, 201534,    502,\n",
       "         2833,   1984,      5,     26,   8906,      3,   4367,   3143,\n",
       "         4785,     21,   5619,      7,   4883,      3,  50058,  16632,\n",
       "           75,     26,  80190,    442,   1870,   4785,   1381,     20,\n",
       "         3451,  15894,     12,     18,  16788,  16632,     64, 201534,\n",
       "           79,    127,     18,   7034,    101,     22,  57299,    621,\n",
       "       356071,      4,    320,    101,     74,   4785,  13876,  11970,\n",
       "           12,     18,     86,   3623,    206,  16632,  10503,  15445,\n",
       "        42210,      6,   1627,   6892,  49424,      5,   1635,     13,\n",
       "            7,  13943,   6458,  12611,    236,   6858,     14,   5929,\n",
       "           29,   9741,    311,    117, 201534,   9958,  41608,  12147,\n",
       "            6,     42, 201534,   1813,   1229,    682,     66,     44,\n",
       "         2895,      3, 137110,      7,    756,  18181,     83, 399999,\n",
       "          661,    809,    285, 363010,    346,     12,     41,     33,\n",
       "           29,  47442], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = positiveFiles[3]\n",
    "maxSeqLength = 250\n",
    "firstFile = np.zeros((maxSeqLength), dtype='int32')\n",
    "with open(fname) as f:\n",
    "    indexCounter = 0\n",
    "    line=f.readline()\n",
    "    cleanedLine = cleanSentences(line)\n",
    "    split = cleanedLine.split()\n",
    "    for word in split:\n",
    "        if indexCounter == len(firstFile):\n",
    "            break\n",
    "        else:\n",
    "            try:\n",
    "                firstFile[indexCounter] = wordsList.index(word)\n",
    "            except ValueError:\n",
    "                firstFile[indexCounter] = 399999 #Vector for unknown words\n",
    "        indexCounter = indexCounter + 1\n",
    "firstFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter=0\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, 'r', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        indexCounter = 0\n",
    "        for word in split:\n",
    "            if indexCounter == len(firstFile):\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                    ids[fileCounter][indexCounter] = 399999 #Vector for unknown words\n",
    "                indexCounter = indexCounter + 1\n",
    "        fileCounter = fileCounter+1\n",
    "        \n",
    "for nf in negativeFiles:\n",
    "    with open(nf, 'r', encoding='utf-8') as f:\n",
    "        line = f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        indexCounter = 0\n",
    "        for word in split:\n",
    "            if indexCounter == len(firstFile):\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    ids[fileCounter][indexCounter] = wordsList.index(word)\n",
    "                except ValueError:\n",
    "                    ids[fileCounter][indexCounter] = 399999 #Vector for unknown words\n",
    "                indexCounter = indexCounter + 1\n",
    "        fileCounter = fileCounter+1\n",
    "\n",
    "np.save('idsMatrix', ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 250)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = np.load('training_data/idsMatrix.npy')\n",
    "ids.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model \n",
    "Lets have a discussion on RNN and LSTM before deep dive in code.\n",
    "\n",
    "NLP data has a temporal aspect to it which means each word in a sentence depends greatly on what came before and comes after it. In order to account for this dependency, we use a recurrent neural network.  \n",
    "\n",
    "The recurrent neural network structure is a little different from the traditional feedforward NN you may be accostumed to seeing. The feedforward network consists of input nodes, hidden units, and output nodes with backpropagation of error to train your network.\n",
    "\n",
    "![caption](Images/SentimentAnalysis17.png)\n",
    "\n",
    "The main difference between feedforward neural networks and recurrent ones is the temporal aspect of the latter. In RNNs, each word in an input sequence will be associated with a specific time step. In effect, the number of time steps will be equal to the max sequence length. \n",
    "\n",
    "![caption](Images/SentimentAnalysis18.png)\n",
    "\n",
    "Associated with each time step is also a new component called a hidden state vector h<sub>t</sub>. From a high level, this vector seeks to encapsulate and summarize all of the information that was seen in the previous time steps. Just like x<sub>t</sub> is a vector that encapsulates all the information of a specific word, h<sub>t</sub> is a vector that summarizes information from previous time steps.\n",
    "\n",
    "The hidden state is a function of both the current word vector and the hidden state vector at the previous time step. The sigma indicates that the sum of the two terms will be put through an activation function (normally a sigmoid or tanh).\n",
    "\n",
    "![caption](Images/SentimentAnalysis15.png)\n",
    "\n",
    "The 2 W terms in the above formulation represent weight matrices. If you take a close look at the superscripts, you’ll see that there’s a weight matrix W<sup>X</sup> which we’re going to multiply with our input, and there’s a recurrent weight matrix W<sup>H</sup> which is multiplied with the hidden state vector at the previous time step. W<sup>H</sup> is a matrix that stays the same across all time steps, and the weight matrix W<sup>X</sup> is different for each input. \n",
    "\n",
    "The magnitude of these weight matrices impact the amount the hidden state vector is affected by either the current vector or the previous hidden state. As an exercise, take a look at the above formula, and consider how h<sub>t</sub> would change if either W<sup>X</sup> or W<sup>H</sup> had large or small values. \n",
    "\n",
    "Let's look at a quick example. When the magnitude of W<sup>H</sup> is large and the magnitude of W<sup>X</sup> is small, we know that h<sub>t</sub> is largely affected by h<sub>t-1</sub> and unaffected by x<sub>t</sub>. In other words, the current hidden state vector sees that the current word is largely inconsequential to the overall summary of the sentence, and thus it will take on mostly the same value as the vector at the previous time step. \n",
    "\n",
    "The weight matrices are updated through an optimization process called backpropagation through time. \n",
    "\n",
    "The hidden state vector at the final time step is fed into a binary softmax classifier where it is multiplied by another weight matrix and put through a softmax function that outputs values between 0 and 1, effectively giving us the probabilities of positive and negative sentiment. \n",
    "\n",
    "![](Images/SentimentAnalysis16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Short Term Memory Units (LSTMs)\n",
    "\n",
    "Long Short Term Memory Units are modules that you can place inside of reucrrent neural entworks. At a high level, they make sure that the hidden state vector h is able to encapsulate information about long term dependencies in the text. As we saw in the previous section, the formulation for h in traditional RNNs is relatively simple. This approach won't be able to effectively connect together information that is separated by more than a couple time steps. We can illiustrate this idea of handling long term dependencies through an example in the field of question answering. The function of question answering models is to take an a passage of text, and answer a question about its content. Let's look at the following example.\n",
    "\n",
    "![caption](Images/SentimentAnalysis4.png)\n",
    "\n",
    "Here, we see that the middle sentence had no impact on the question that was asked. However, there is a strong connection between the first and third sentences. With a classic RNN, the hidden state vector at the end of the network might have stored more information about the dog sentence than about the first sentence about the number. Basically, the addition of LSTM units make it possible to determine the correct and useful information that needs to be stored in the hidden state vector.\n",
    "\n",
    "Looking at LSTM units from a more technical viewpoint, the units take in the current word vector x<sub>t</sub> and output the hidden state vector h<sub>t</sub>. In these units, the formulation for h<sub>t</sub> will be a bit more complex than that in a typical RNN. The computation is broken up into 4 components, an input gate, a forget gate, an output gate, and a new memory container. \n",
    "\n",
    "![caption](Images/SentimentAnalysis10.png)\n",
    "\n",
    "Each gate will take in x<sub>t</sub> and h<sub>t-1</sub> (not shown in image) as inputs and will perform some computation on them to obtain intermediate states. Each intermediate state gets fed into different pipelines and eventually the information is aggregated to form h<sub>t</sub>. For simplicity sake, we won't go into the specific formulations for each gate, but it's worth noting that each of these gates can be thought of as different modules within the LSTM that each have different functions. The input gate determines how much emphasis to put on each of the inputs, the forget gate determines the information that we'll throw away, and the output gate determines the final h<sub>t</sub> based on the intermediate states. For more information on understanding the functions of the different gates and the full equations, check out Christopher Olah's great [blog post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n",
    "\n",
    "Looking back at the first example with question “What is the sum of the two numbers?”, the model would have to be trained on similar types of questions and answers. The LSTM units would then be able to realize that any sentence without numbers will likely not have an impact on the answer to the question, and thus the unit will be able to utilize its forget gate to discard the unnecessary information about the dog, and rather keep the information regarding the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are ready to create tensorflow graph.\n",
    "\n",
    "We will specify two placeholders, one for inputs into the network and one for the lables. \n",
    "\n",
    "Label placehodler represent a set of values either [1,0] or [0,1] depending on each training example is positive or negative.\n",
    "Each row in input placeholder represent the integrized representation of each words for the line/training example that we include in our batch.\n",
    "\n",
    "![caption](Images/SentimentAnalysis12.png)\n",
    "\n",
    "\n",
    "Once we have get input data placeholder, we are going to call tf.nn.lookup() function, to get word vector resprestation for our examples.\n",
    "This will return us a 3-D tensor of dimensionality - batch size (10 lines/examples), maxSequenceLength (250 average words in a line) and vector representation of each word (vector represtation of a word is 1, 50).\n",
    "\n",
    "![caption](Images/SentimentAnalysis13.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "numDimensions = 50\n",
    "maxSeqLength = 250\n",
    "batchSize = 24\n",
    "lstmUnits = 64\n",
    "numClasses = 2\n",
    "iterations = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "labels = tf.placeholder(tf.float32, [batchSize, numClasses])\n",
    "input_data = tf.placeholder(tf.int32, [batchSize, maxSeqLength])\n",
    "\n",
    "data = tf.Variable(tf.zeros([batchSize, maxSeqLength, numDimensions]),dtype=tf.float32)\n",
    "data = tf.nn.embedding_lookup(wordVectors,input_data)\n",
    "\n",
    "lstmCell = tf.contrib.rnn.BasicLSTMCell(lstmUnits)\n",
    "lstmCell = tf.contrib.rnn.DropoutWrapper(cell=lstmCell, output_keep_prob=0.25)\n",
    "value, _ = tf.nn.dynamic_rnn(lstmCell, data, dtype=tf.float32)\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([lstmUnits, numClasses]))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[numClasses]))\n",
    "value = tf.transpose(value, [1, 0, 2])\n",
    "last = tf.gather(value, int(value.get_shape()[0]) - 1)\n",
    "prediction = (tf.matmul(last, weight) + bias)\n",
    "\n",
    "correctPred = tf.equal(tf.argmax(prediction,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correctPred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: Tensor(\"Placeholder:0\", shape=(24, 2), dtype=float32)\n",
      "Input_data: Tensor(\"Placeholder_1:0\", shape=(24, 250), dtype=int32)\n",
      "Data: Tensor(\"embedding_lookup:0\", shape=(24, 250, 50), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(24), Dimension(250), Dimension(50)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Labels:\", labels)\n",
    "print(\"Input_data:\", input_data)\n",
    "print(\"Data:\", data)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment out optimizer and loss parameters after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=labels))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "tf.summary.scalar('Loss', loss)\n",
    "tf.summary.scalar('Accuracy', accuracy)\n",
    "merged = tf.summary.merge_all()\n",
    "logdir = \"tensorboard/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+'/'\n",
    "writer = tf.summary.FileWriter(logdir, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def getTrainBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        if (i % 2 == 0): \n",
    "            num = randint(1,11499)\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            num = randint(13499,24999)\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels\n",
    "\n",
    "def getTestBatch():\n",
    "    labels = []\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    for i in range(batchSize):\n",
    "        num = randint(11499,13499)\n",
    "        if (num <= 12499):\n",
    "            labels.append([1,0])\n",
    "        else:\n",
    "            labels.append([0,1])\n",
    "        arr[i] = ids[num-1:num]\n",
    "    return arr, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(iterations):\n",
    "    nextBatch, nextBatchLabels = getTrainBatch();\n",
    "    sess.run(optimizer, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "   \n",
    "    #Write summary to Tensorboard\n",
    "    if (i % 50 == 0):\n",
    "        summary = sess.run(merged, {input_data: nextBatch, labels: nextBatchLabels})\n",
    "        writer.add_summary(summary, i)\n",
    "    #Save the network every 10,000 training iterations\n",
    "    if (i % 10000 == 0 and i != 0):\n",
    "        save_path = saver.save(sess, \"models/pretrained_lstm.ckpt\", global_step=i)\n",
    "        print(\"saved to %s\" % save_path)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/pretrained_lstm.ckpt-90000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, tf.train.latest_checkpoint('models'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentenceMatrix(sentence):\n",
    "    arr = np.zeros([batchSize, maxSeqLength])\n",
    "    sentenceMatrix = np.zeros([batchSize,maxSeqLength], dtype='int32')\n",
    "    cleanedSentence = cleanSentences(sentence)\n",
    "    split = cleanedSentence.split()\n",
    "    for indexCounter,word in enumerate(split):\n",
    "        try:\n",
    "            sentenceMatrix[0,indexCounter] = wordsList.index(word)\n",
    "        except ValueError:\n",
    "            sentenceMatrix[0,indexCounter] = 399999 #Vector for unkown words\n",
    "    return sentenceMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 250)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 47, 978,  47, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputText = \"Its bad its good. and its lovely and car was awesome\"\n",
    "inputMatrix = getSentenceMatrix(inputText)\n",
    "print(inputMatrix.shape)\n",
    "inputMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.65980315  3.62634802]\n",
      "negative sentiment\n"
     ]
    }
   ],
   "source": [
    "predictedSentiment = sess.run(prediction, {input_data: inputMatrix})[0]\n",
    "\n",
    "print(predictedSentiment)\n",
    "if (predictedSentiment[0]>predictedSentiment[1]):\n",
    "    print(\"Positive Sentiment\")\n",
    "else:\n",
    "    print(\"negative sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis using Keras in progress..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple LSTM with glove embeddings and two dense layers\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation, SpatialDropout1D\n",
    "\n",
    "nextBatch, nextBatchLabels = getTrainBatch();\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     50,\n",
    "                     weights=[next_batch],\n",
    "                     input_length=max_len,\n",
    "                     trainable=False))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(LSTM(100, dropout=0.3, recurrent_dropout=0.3))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextBatch, nextBatchLabels = getTrainBatch();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nextBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
